{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2fe081",
   "metadata": {},
   "source": [
    "## Merge dataset to a week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1925b3",
   "metadata": {},
   "source": [
    "### Import libaraies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8442ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212fe852",
   "metadata": {},
   "source": [
    "### Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd5eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the day according to the filename(date)\n",
    "def day_calculator(s):\n",
    "    s = s.split(\".\")[0]\n",
    "    year, month, date = s.split(\"-\")\n",
    "    res = datetime.datetime(int(year), int(month), int(date))\n",
    "    return res.strftime(\"%a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "011ae020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the merge step and export as csv file\n",
    "def merge_missing(df, df_new, date):\n",
    "    missing_rows = df_new[~df_new['uid'].isin(df['uid'])]\n",
    "    file_name = date + \".csv\"\n",
    "    if len(missing_rows) == 0:\n",
    "        return False, df\n",
    "    else:\n",
    "        df_update = pd.concat([df, missing_rows])\n",
    "        df_update.to_csv(file_name, index = False)\n",
    "        return True, df_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d8c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of users after merge and their uid\n",
    "def find_merge_amount(dict_merge, date):\n",
    "    dict_mon = dict_merge[date].copy()\n",
    "\n",
    "    df_mon1 = pd.DataFrame(columns=['uid', 't', 'lat', 'long'])\n",
    "    df_mon2 = pd.DataFrame(columns=['uid', 't', 'lat', 'long'])\n",
    "    full = 0\n",
    "    idx = 0\n",
    "    repeated = dict()\n",
    "    while len(dict_mon) > 0: \n",
    "        if len(df_mon1) < 100 * 48:   \n",
    "            file = dict_mon[idx]\n",
    "            df = pd.read_csv('data/' + file)\n",
    "            flag, df_mon1 = merge_missing(df_mon1, df, date)\n",
    "            dict_mon.remove(file)\n",
    "            users = df['uid'].unique()\n",
    "            for user in users:\n",
    "                if user not in repeated.keys():\n",
    "                    repeated[user] = 1\n",
    "                else:\n",
    "                    repeated[user] += 1\n",
    "    \n",
    "    print(date, \":\", len(repeated))\n",
    "    \n",
    "    contained_users = list(repeated.keys())\n",
    "\n",
    "    return contained_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1e31f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the common users for a week\n",
    "def find_occurrence(arr):\n",
    "    count = 0\n",
    "    dict_count = dict()\n",
    "    for ele in arr:\n",
    "        if ele not in dict_count.keys():\n",
    "            dict_count[ele] = 1\n",
    "        else:\n",
    "            dict_count[ele] += 1\n",
    "            \n",
    "    for user in dict_count.keys():\n",
    "        if dict_count[user] >= 7:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523ee1d",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5617ef04",
   "metadata": {},
   "source": [
    "Read and find the exact day for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d47e856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2008-05-30.csv', '2008-10-06.csv', '2008-10-25.csv', '2008-10-26.csv', '2008-10-28.csv', '2008-10-29.csv', '2008-10-30.csv', '2008-10-31.csv', '2008-11-01.csv', '2008-11-02.csv', '2008-11-03.csv', '2008-11-04.csv', '2008-11-05.csv', '2008-11-06.csv', '2008-11-07.csv', '2008-11-08.csv', '2008-11-09.csv', '2008-11-10.csv', '2008-11-12.csv', '2008-11-13.csv', '2008-11-14.csv', '2008-11-15.csv', '2008-11-16.csv', '2008-11-18.csv', '2008-11-21.csv', '2008-12-03.csv', '2008-12-04.csv', '2008-12-05.csv', '2008-12-06.csv', '2008-12-07.csv', '2008-12-09.csv', '2008-12-10.csv', '2008-12-11.csv', '2008-12-12.csv', '2008-12-13.csv', '2008-12-14.csv', '2008-12-19.csv', '2009-01-09.csv', '2009-01-16.csv', '2009-02-07.csv', '2009-02-10.csv', '2009-02-13.csv', '2009-02-14.csv', '2009-02-15.csv', '2009-02-16.csv', '2009-02-17.csv', '2009-02-18.csv', '2009-02-19.csv', '2009-02-20.csv', '2009-02-21.csv', '2009-02-22.csv', '2009-02-24.csv', '2009-02-25.csv', '2009-02-26.csv', '2009-03-05.csv', '2009-03-06.csv', '2009-03-07.csv', '2009-03-08.csv', '2009-03-09.csv', '2009-03-10.csv', '2009-03-11.csv', '2009-03-12.csv', '2009-03-13.csv', '2009-03-14.csv', '2009-03-15.csv', '2009-03-17.csv', '2009-03-18.csv', '2009-03-19.csv', '2009-03-20.csv', '2009-03-21.csv', '2009-03-22.csv', '2009-03-23.csv', '2009-04-02.csv', '2009-04-03.csv', '2009-04-04.csv', '2009-04-05.csv', '2009-04-06.csv', '2009-04-07.csv', '2009-04-08.csv', '2009-04-09.csv', '2009-04-10.csv', '2009-04-11.csv', '2009-04-12.csv', '2009-04-13.csv', '2009-04-14.csv', '2009-04-15.csv', '2009-04-16.csv', '2009-04-17.csv', '2009-04-18.csv', '2009-04-19.csv', '2009-04-20.csv', '2009-05-01.csv', '2009-05-25.csv', '2009-05-28.csv', '2009-06-06.csv', '2009-06-09.csv', '2009-06-13.csv', '2009-06-14.csv', '2009-06-16.csv', '2009-06-20.csv', '2009-06-21.csv', '2009-06-23.csv', '2009-07-02.csv', '2009-07-03.csv', '2009-07-07.csv', '2009-07-09.csv', '2009-07-11.csv', '2009-07-15.csv']\n",
      "['Fri', 'Mon', 'Sat', 'Sun', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'Mon', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'Tue', 'Fri', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'Fri', 'Fri', 'Fri', 'Sat', 'Tue', 'Fri', 'Sat', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'Tue', 'Wed', 'Thu', 'Thu', 'Fri', 'Sat', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'Mon', 'Thu', 'Fri', 'Sat', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'Mon', 'Fri', 'Mon', 'Thu', 'Sat', 'Tue', 'Sat', 'Sun', 'Tue', 'Sat', 'Sun', 'Tue', 'Thu', 'Fri', 'Tue', 'Thu', 'Sat', 'Wed']\n"
     ]
    }
   ],
   "source": [
    "filenames = os.listdir(r'data')\n",
    "print(filenames)\n",
    "dates = list(map(day_calculator, filenames))\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9cb21",
   "metadata": {},
   "source": [
    "Find the number of each day and combine them by a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d65a56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon = 10\n",
      "Tue = 15\n",
      "Wed = 12\n",
      "Thu = 16\n",
      "Fri = 20\n",
      "Sat = 19\n",
      "Sun = 16\n"
     ]
    }
   ],
   "source": [
    "all_dates = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "for day in all_dates:\n",
    "    count_day = dates.count(day)\n",
    "    print(day, \"=\", count_day)\n",
    "    \n",
    "dict_merge = {}\n",
    "for day in all_dates:\n",
    "    dict_merge[day] = []\n",
    "    for i in range(len(filenames)):\n",
    "        if dates[i] == day:\n",
    "            dict_merge[day].append(filenames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5dc941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon : 49\n",
      "Tue : 47\n",
      "Wed : 45\n",
      "Thu : 50\n",
      "Fri : 62\n",
      "Sat : 58\n",
      "Sun : 50\n",
      "overall: 32\n"
     ]
    }
   ],
   "source": [
    "# Check user number\n",
    "all_users = []\n",
    "\n",
    "for date in all_dates:\n",
    "    contained_users = find_merge_amount(dict_merge, date)\n",
    "\n",
    "    if all_users == []:\n",
    "        all_users = contained_users\n",
    "    else:\n",
    "        all_users = all_users + contained_users\n",
    "\n",
    "# print(all_users)\n",
    "print(\"overall:\", find_occurrence(all_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642af66b",
   "metadata": {},
   "source": [
    "### Switch time and concatenate with the next day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db44b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_time(df_prev, df_next):\n",
    "    user_prev = sorted(list(df_prev['uid'].drop_duplicates()))\n",
    "    user_next = sorted(list(df_next['uid'].drop_duplicates()))\n",
    "    users = []\n",
    "    \n",
    "    for user in user_prev:\n",
    "        if user in user_next:\n",
    "            users.append(user)\n",
    "            \n",
    "    df_update = pd.DataFrame(columns=['uid', 't', 'lat', 'long'])\n",
    "    \n",
    "    for user in users:\n",
    "        prev_day = df_prev[(df_prev['uid'] == user) & (df_prev['t'] >= 12) & (df_prev['t'] < 48)]\n",
    "        next_morning = df_next[(df_next['uid'] == user) & (df_next['t'] >= 0) & (df_next['t'] < 12)]\n",
    "        df_update = pd.concat([df_update, prev_day])\n",
    "        df_update = pd.concat([df_update, next_morning])\n",
    "        \n",
    "    print(\"user amount:\", len(users))\n",
    "    \n",
    "    return df_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "279f7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'data_by_day/'\n",
    "all_dates = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "# import datasets\n",
    "df_Mon = pd.read_csv(filepath + 'Mon.csv')\n",
    "df_Tue = pd.read_csv(filepath + 'Tue.csv')\n",
    "df_Wed = pd.read_csv(filepath + 'Wed.csv')\n",
    "df_Thu = pd.read_csv(filepath + 'Thu.csv')\n",
    "df_Fri = pd.read_csv(filepath + 'Fri.csv')\n",
    "df_Sat = pd.read_csv(filepath + 'Sat.csv')\n",
    "df_Sun = pd.read_csv(filepath + 'Sun.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06d660da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user amount: 39\n",
      "user amount: 38\n",
      "user amount: 41\n",
      "user amount: 42\n",
      "user amount: 44\n",
      "user amount: 52\n",
      "user amount: 48\n"
     ]
    }
   ],
   "source": [
    "df_Mon_update = switch_time(df_Sun, df_Mon)\n",
    "df_Tue_update = switch_time(df_Mon, df_Tue)\n",
    "df_Wed_update = switch_time(df_Tue, df_Wed)\n",
    "df_Thu_update = switch_time(df_Wed, df_Thu)\n",
    "df_Fri_update = switch_time(df_Thu, df_Fri)\n",
    "df_Sat_update = switch_time(df_Fri, df_Sat)\n",
    "df_Sun_update = switch_time(df_Sat, df_Sun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6fdab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Mon_update.to_csv(\"data_by_day/Mon_update.csv\", index = False)\n",
    "df_Tue_update.to_csv(\"data_by_day/Tue_update.csv\", index = False)\n",
    "df_Wed_update.to_csv(\"data_by_day/Wed_update.csv\", index = False)\n",
    "df_Thu_update.to_csv(\"data_by_day/Thu_update.csv\", index = False)\n",
    "df_Fri_update.to_csv(\"data_by_day/Fri_update.csv\", index = False)\n",
    "df_Sat_update.to_csv(\"data_by_day/Sat_update.csv\", index = False)\n",
    "df_Sun_update.to_csv(\"data_by_day/Sun_update.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cac124",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29e70215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random(upper, num):\n",
    "    res = set()\n",
    "    while len(res) < num:\n",
    "        n = random.randint(0, upper)\n",
    "        res.add(n)\n",
    "    return res\n",
    "\n",
    "def plot_dates(trajectory, date):\n",
    "    m = folium.Map(location=[39.9, 116.4], zoom_start=9, tiles=\"cartodb positron\")\n",
    "    colors = ['gray', 'black', 'lightred', 'cadetblue', 'red', 'darkblue', 'green', 'lightblue', 'beige', 'blue', 'lightgreen', 'white', 'orange', 'darkgreen', 'purple', 'darkred', 'pink', 'darkpurple', 'lightgray']\n",
    "    save_file_name = date + \".html\"\n",
    "    \n",
    "    index = create_random(len(trajectory) - 1, 5)\n",
    "    \n",
    "    for i in index:\n",
    "        for location in range(len(trajectory[i])):\n",
    "            popup = f\"{location+1}\"\n",
    "            loc = f\"{location+1}\"\n",
    "            folium.Marker(location=trajectory[i][location],\n",
    "                         tooltip=popup,\n",
    "                         icon=folium.Icon(icon=loc, prefix='fa', color=colors[i])\n",
    "                         ).add_to(m)\n",
    "            \n",
    "        folium.PolyLine(trajectory[i], color=\"red\", weight=2.5, opacity=1).add_to(m)\n",
    "    m.save(save_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69061670",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = []\n",
    "daily_trajectory = []\n",
    "current_date = dict_merge['Mon']\n",
    "\n",
    "for ele in current_date:\n",
    "    df = pd.read_csv('data/'+ele)\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if row['uid'] == 3:\n",
    "            daily_trajectory.append((row['lat'], row['long']))\n",
    "    trajectory.append(daily_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c884e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dates(trajectory, \"Mon\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

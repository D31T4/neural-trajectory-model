{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on Shanghai Telecom Dataset\n",
    "\n",
    "Dataset discretize with 50 Gaussian mixture clusters (mog_50.npy), 30 minute interval. Unfiltered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- Pre-processed dataset with 50 clusters located at `{ROOT}/data/sh30-c50`\n",
    "- Pre-computed 50 clusters located at `{ROOT}/data/exploratory_analysis/mog_50.npy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.path import ROOT\n",
    "from src.ml.checkpoint import Checkpoint\n",
    "\n",
    "# trajectory length\n",
    "SEQ_LENGTH: int = 48\n",
    "\n",
    "# cuda flag\n",
    "USE_CUDA: bool = True\n",
    "\n",
    "if USE_CUDA and not torch.cuda.is_available():\n",
    "    USE_CUDA = False\n",
    "    print('fallback to cpu as CUDA is not available on this device')\n",
    "\n",
    "CHECKPOINT_PREFIX: str = 'sh30-c50'\n",
    "CACHE_PREFIX: str = 'sh30-c50'\n",
    "\n",
    "checkpoint = Checkpoint(\n",
    "    checkpoint_interval=5,\n",
    "    prefix=CHECKPOINT_PREFIX\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define path\n",
    "\n",
    "Change the path variable here if you place your dataset files in a different location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_path = f'{ROOT}/exploratory_analysis/mog_50.npy'\n",
    "dataset_path = str(ROOT.joinpath('data/sh30-c50'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split dataset\n",
    "\n",
    "Split to pre-defined training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "from src.data_preprocess.trajectory import from_dataframe\n",
    "from src.ml.dataset import get_shanghai_date\n",
    "\n",
    "file_list = os.listdir(dataset_path)\n",
    "\n",
    "def is_test(fname: str):\n",
    "    '''\n",
    "    returns True if file belongs to test set\n",
    "    '''\n",
    "    fdate = get_shanghai_date(fname)\n",
    "    ref_date = date(2014, 6, 18)\n",
    "    return fdate >= ref_date and (fdate - ref_date).days < 15\n",
    "\n",
    "\n",
    "test_files = [fname for fname in file_list if is_test(fname)]\n",
    "train_files = [fname for fname in file_list if not is_test(fname)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read basestations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.dataset import create_point_to_class_map\n",
    "\n",
    "all_candidates = torch.tensor(np.load(cluster_path), dtype=torch.float32)\n",
    "\n",
    "point_to_class_map = create_point_to_class_map(all_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset\n",
    "\n",
    "Load dataset files into in-memory tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "from src.ml.dataset import TrajectoryDataset, get_shanghai_date, CACHE_PATH\n",
    "\n",
    "def read_file(fname: str):\n",
    "    df = pd.read_csv(f'{dataset_path}/{fname}')\n",
    "    return get_shanghai_date(fname), [*from_dataframe(df, SEQ_LENGTH).values()]\n",
    "    \n",
    "\n",
    "train_set = TrajectoryDataset(sequence_length=SEQ_LENGTH, point_to_class_map=point_to_class_map)\n",
    "\n",
    "if os.path.exists(f'{CACHE_PATH}/{CACHE_PREFIX}_train_data.pt'):\n",
    "    train_set.load(f'{CACHE_PATH}/{CACHE_PREFIX}_train_data.pt')\n",
    "else:\n",
    "    train_set.read_files(\n",
    "        train_files,\n",
    "        read_file=read_file\n",
    "    )\n",
    "\n",
    "    train_set.save(f'{CACHE_PATH}/{CACHE_PREFIX}_train_data.pt')\n",
    "\n",
    "# fix seed for reproducibility\n",
    "train_set, valid_set = random_split(train_set, [0.8, 0.2], torch.Generator().manual_seed(123))\n",
    "\n",
    "test_set = TrajectoryDataset(sequence_length=SEQ_LENGTH, point_to_class_map=point_to_class_map)\n",
    "\n",
    "if os.path.exists(f'{CACHE_PATH}/{CACHE_PREFIX}_test_data.pt'):\n",
    "    test_set.load(f'{CACHE_PATH}/{CACHE_PREFIX}_test_data.pt')\n",
    "else:\n",
    "    test_set.read_files(\n",
    "        test_files,\n",
    "        read_file=read_file\n",
    "    )\n",
    "\n",
    "    test_set.save(f'{CACHE_PATH}/{CACHE_PREFIX}_test_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pre-process pipeline\n",
    "\n",
    "1. convert to Cartesian coordinates by tangent plane project. Choose center of plane (reference point) to be median of lat-long.\n",
    "2. normalize to [-1, +1] for better gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.utils import create_shanghai_preprocessor, to_cartesian\n",
    "\n",
    "ref_lat = all_candidates[:, 0].median()\n",
    "ref_long = all_candidates[:, 1].median()\n",
    "\n",
    "all_candidates_cart = to_cartesian(all_candidates, ref_point=(ref_lat, ref_long))\n",
    "min_x, max_x = all_candidates_cart[:, 0].min().item(), all_candidates_cart[:, 0].max().item()\n",
    "min_y, max_y = all_candidates_cart[:, 1].min().item(), all_candidates_cart[:, 1].max().item()\n",
    "del all_candidates_cart\n",
    "\n",
    "preprocess = create_shanghai_preprocessor(\n",
    "    x_range=(min_x, max_x),\n",
    "    y_range=(min_y, max_y),\n",
    "    ref_point=(ref_lat, ref_long)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.model import TrajectoryModel\n",
    "from src.ml.model.modules import TransformerTrajectoryEncoder, BaseStationEmbedding\n",
    "\n",
    "model_dim = 128\n",
    "\n",
    "base_station_embedding = BaseStationEmbedding(\n",
    "    feat_dim=(2, 64),\n",
    "    context_dim=(31, 48),\n",
    "    out_dim=model_dim,\n",
    "    layer_norm=True\n",
    ")\n",
    "\n",
    "trajectory_encoder = TransformerTrajectoryEncoder(\n",
    "    in_dim=model_dim,\n",
    "    max_len=SEQ_LENGTH,\n",
    "    hid_dim=(model_dim, model_dim * 2, 8),\n",
    "    do_prob=0.2,\n",
    "    n_blocks=4,\n",
    ")\n",
    "\n",
    "model = TrajectoryModel(\n",
    "    base_station_embedding=base_station_embedding,\n",
    "    trajectory_encoder=trajectory_encoder,\n",
    ")\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define train config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.config import TrainConfig\n",
    "\n",
    "\n",
    "config = TrainConfig(\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    datasets={ 'train': train_set, 'valid': valid_set },\n",
    "    n_epoch=5,\n",
    "    all_candidates=all_candidates,\n",
    "    verbose=True,\n",
    "    cuda=USE_CUDA,\n",
    "    checkpoint=checkpoint,\n",
    "    preprocess=preprocess,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] 1:   0%|          | 0/6382 [00:00<?, ?it/s]c:\\Users\\chinp\\source\\capstone-project-9900w16aaiadvance\\venv\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "[train] 1: 100%|██████████| 6382/6382 [06:31<00:00, 16.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.24424185953037336\n",
      "elapsed: 391.3507311344147\n",
      "perplexity: 1.276653063863371\n",
      "accuracy: 0.9418718168939268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[valid] 1: 100%|██████████| 1596/1596 [00:45<00:00, 34.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdev: 1069.5187544667333\n",
      "elapsed: 45.88335824012756\n",
      "perplexity: 1.2138840117777863\n",
      "accuracy: 0.9506877033707491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test] 1: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 0.0019943714141845703\n",
      "perplexity: 1.0\n",
      "accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] 2: 100%|██████████| 6382/6382 [05:18<00:00, 20.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.18692150207563876\n",
      "elapsed: 318.82864141464233\n",
      "perplexity: 1.2055326495248897\n",
      "accuracy: 0.9508942829445588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[valid] 2: 100%|██████████| 1596/1596 [01:05<00:00, 24.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdev: 1452.8843864115856\n",
      "elapsed: 65.24112844467163\n",
      "perplexity: 1.2056378325640167\n",
      "accuracy: 0.9507650045075811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test] 2: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 1.0\n",
      "accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] 3: 100%|██████████| 6382/6382 [07:38<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.1832659491812107\n",
      "elapsed: 458.4678075313568\n",
      "perplexity: 1.2011338061593921\n",
      "accuracy: 0.9510706184482695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[valid] 3: 100%|██████████| 1596/1596 [01:35<00:00, 16.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdev: 775.1298095588397\n",
      "elapsed: 95.85079169273376\n",
      "perplexity: 1.2020643790365675\n",
      "accuracy: 0.9511805474758148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test] 3: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 0.0029897689819335938\n",
      "perplexity: 1.0\n",
      "accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] 4: 100%|██████████| 6382/6382 [08:13<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.1814150206431798\n",
      "elapsed: 493.90528559684753\n",
      "perplexity: 1.19891264955478\n",
      "accuracy: 0.9511751377705832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[valid] 4: 100%|██████████| 1596/1596 [00:55<00:00, 28.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdev: 725.3774749449918\n",
      "elapsed: 55.32203269004822\n",
      "perplexity: 1.1995702063451483\n",
      "accuracy: 0.9512492816400409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test] 4: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 0.001993417739868164\n",
      "perplexity: 1.0\n",
      "accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] 5: 100%|██████████| 6382/6382 [06:04<00:00, 17.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.1804899097548985\n",
      "elapsed: 364.9326288700104\n",
      "perplexity: 1.1978040352832109\n",
      "accuracy: 0.9511816588754902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[valid] 5: 100%|██████████| 1596/1596 [01:15<00:00, 21.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdev: 768.3676745574875\n",
      "elapsed: 75.61929893493652\n",
      "perplexity: 1.1987064280074258\n",
      "accuracy: 0.9511579072340987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test] 5: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 0.0019941329956054688\n",
      "perplexity: 1.0\n",
      "accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.ml.train import train\n",
    "\n",
    "if USE_CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "state = train(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "\n",
    "check accuracy if we just predict the next position as the last known position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[valid] 6: 100%|██████████| 1596/1596 [00:36<00:00, 44.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9712252526592955\n",
      "152.37281795445628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from src.ml.utils import haversine\n",
    "\n",
    "valid_count = 0\n",
    "valid_acc = 0\n",
    "valid_mse = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for trajectories, context, target in tqdm.tqdm(state.valid_loader, desc=state.get_tqdm_desc('[valid]'), disable=not config.verbose):\n",
    "        batch_size, sequence_length = trajectories.shape[:2]\n",
    "    \n",
    "        context: torch.FloatTensor = context\n",
    "        trajectories: torch.FloatTensor = trajectories\n",
    "        target: torch.IntTensor = target\n",
    "\n",
    "        if config.cuda:\n",
    "            context = context.cuda()\n",
    "            trajectories = trajectories.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        if config.preprocess:\n",
    "            trajectories = config.preprocess(trajectories)\n",
    "\n",
    "        trajectories = torch.concat((trajectories[:, :1, :], trajectories), dim=1)\n",
    "\n",
    "        # accumulate accuracy\n",
    "        acc = (trajectories[:, :-1] == trajectories[:, 1:]).prod(dim=-1).float().mean().item()\n",
    "        valid_acc += batch_size * acc\n",
    "\n",
    "        mdev = haversine(trajectories[:, 1:], trajectories[:, :-1]).mean().item()\n",
    "        valid_mse += batch_size * mdev\n",
    "\n",
    "        valid_count += batch_size\n",
    "\n",
    "print(valid_acc / valid_count)\n",
    "print(valid_mse / valid_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "| Model | Perplexity | Mean Error (m) | Accuracy |\n",
    "| ----- | ---------- | -------------- | -------- |\n",
    "| 100 clusters | 1.24 | 721 | 0.9464692816387501 |\n",
    "| 50 clusters | 768 | 0.9511579072340987 |\n",
    "| last position | - | 152 | 0.9712252526592955 | "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

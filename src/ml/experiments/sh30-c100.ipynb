{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on Shanghai Telecom Dataset\n",
    "\n",
    "Dataset discretize with 100 Gaussian mixture clusters (mog_100.npy), 30 minute interval. Unfiltered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- Pre-processed dataset with 100 clusters located at `{ROOT}/data/sh30-c100`\n",
    "- Pre-processed dataset with 50 clusters located at `{ROOT}/data/sh30-c50`\n",
    "- Pre-computed 50 clusters located at `{ROOT}/data/exploratory_analysis/mog_50.npy`\n",
    "- Pre-computed 100 clusters located at `{ROOT}/data/exploratory_analysis/mog_100.npy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tqdm\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from src.path import ROOT\n",
    "from src.ml.checkpoint import Checkpoint\n",
    "\n",
    "# trajectory length\n",
    "SEQ_LENGTH: int = 48\n",
    "\n",
    "# cuda flag\n",
    "USE_CUDA: bool = True\n",
    "\n",
    "if USE_CUDA and not torch.cuda.is_available():\n",
    "    USE_CUDA = False\n",
    "    print('fallback to cpu as CUDA is not available on this device')\n",
    "\n",
    "CHECKPOINT_PREFIX: str = 'sh30-c100'\n",
    "CACHE_PREFIX: str = 'sh30-c100'\n",
    "\n",
    "checkpoint = Checkpoint(\n",
    "    checkpoint_interval=5,\n",
    "    prefix=CHECKPOINT_PREFIX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define path\n",
    "\n",
    "Change the path variable here if you place your dataset files in a different location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_path = f'{ROOT}/exploratory_analysis/mog_100.npy'\n",
    "dataset_path = str(ROOT.joinpath('data/sh30-c100'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split dataset\n",
    "\n",
    "Split to pre-defined training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "from src.data_preprocess.trajectory import from_dataframe\n",
    "from src.ml.dataset import get_shanghai_date\n",
    "\n",
    "file_list = os.listdir(dataset_path)\n",
    "\n",
    "def is_test(fname: str):\n",
    "    '''\n",
    "    returns True if file belongs to test set\n",
    "    '''\n",
    "    fdate = get_shanghai_date(fname)\n",
    "    ref_date = date(2014, 6, 18)\n",
    "    return fdate >= ref_date and (fdate - ref_date).days < 15\n",
    "\n",
    "\n",
    "test_files = [fname for fname in file_list if is_test(fname)]\n",
    "train_files = [fname for fname in file_list if not is_test(fname)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read basestations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.dataset import create_point_to_class_map\n",
    "\n",
    "all_candidates = torch.tensor(np.load(cluster_path), dtype=torch.float32)\n",
    "\n",
    "point_to_class_map = create_point_to_class_map(all_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset\n",
    "\n",
    "Load dataset files into in-memory tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "from src.ml.dataset import TrajectoryDataset, get_shanghai_date, CACHE_PATH\n",
    "\n",
    "def read_file(fname: str):\n",
    "    df = pd.read_csv(f'{dataset_path}/{fname}')\n",
    "    return get_shanghai_date(fname), [*from_dataframe(df, SEQ_LENGTH).values()]\n",
    "    \n",
    "\n",
    "train_set = TrajectoryDataset(sequence_length=SEQ_LENGTH, point_to_class_map=point_to_class_map)\n",
    "\n",
    "if os.path.exists(f'{CACHE_PATH}/{CACHE_PREFIX}_train_data.pt'):\n",
    "    train_set.load(f'{CACHE_PATH}/{CACHE_PREFIX}_train_data.pt')\n",
    "else:\n",
    "    train_set.read_files(\n",
    "        train_files,\n",
    "        read_file=read_file\n",
    "    )\n",
    "\n",
    "    train_set.save(f'{CACHE_PATH}/{CACHE_PREFIX}_train_data.pt')\n",
    "\n",
    "# fix seed for reproducibility\n",
    "train_set, valid_set = random_split(train_set, [0.8, 0.2], torch.Generator().manual_seed(123))\n",
    "\n",
    "test_set = TrajectoryDataset(sequence_length=SEQ_LENGTH, point_to_class_map=point_to_class_map)\n",
    "\n",
    "if os.path.exists(f'{CACHE_PATH}/{CACHE_PREFIX}_test_data.pt'):\n",
    "    test_set.load(f'{CACHE_PATH}/{CACHE_PREFIX}_test_data.pt')\n",
    "else:\n",
    "    test_set.read_files(\n",
    "        test_files,\n",
    "        read_file=read_file\n",
    "    )\n",
    "\n",
    "    test_set.save(f'{CACHE_PATH}/{CACHE_PREFIX}_test_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pre-process pipeline\n",
    "\n",
    "1. convert to Cartesian coordinates by tangent plane project. Choose center of plane (reference point) to be median of lat-long.\n",
    "2. normalize to [-1, +1] for better gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.utils import create_shanghai_preprocessor, to_cartesian\n",
    "\n",
    "ref_lat = all_candidates[:, 0].median()\n",
    "ref_long = all_candidates[:, 1].median()\n",
    "\n",
    "all_candidates_cart = to_cartesian(all_candidates, ref_point=(ref_lat, ref_long))\n",
    "min_x, max_x = all_candidates_cart[:, 0].min().item(), all_candidates_cart[:, 0].max().item()\n",
    "min_y, max_y = all_candidates_cart[:, 1].min().item(), all_candidates_cart[:, 1].max().item()\n",
    "del all_candidates_cart\n",
    "\n",
    "preprocess = create_shanghai_preprocessor(\n",
    "    x_range=(min_x, max_x),\n",
    "    y_range=(min_y, max_y),\n",
    "    ref_point=(ref_lat, ref_long)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.model import TrajectoryModel\n",
    "from src.ml.model.modules import TransformerTrajectoryEncoder, BaseStationEmbedding\n",
    "\n",
    "model_dim = 128\n",
    "\n",
    "base_station_embedding = BaseStationEmbedding(\n",
    "    feat_dim=(2, 64),\n",
    "    context_dim=(31, 48),\n",
    "    out_dim=model_dim,\n",
    "    layer_norm=True\n",
    ")\n",
    "\n",
    "trajectory_encoder = TransformerTrajectoryEncoder(\n",
    "    in_dim=model_dim,\n",
    "    max_len=SEQ_LENGTH,\n",
    "    hid_dim=(model_dim, model_dim * 2, 8),\n",
    "    do_prob=0.2,\n",
    "    n_blocks=4,\n",
    "    #layer_norm=True\n",
    ")\n",
    "\n",
    "model = TrajectoryModel(\n",
    "    base_station_embedding=base_station_embedding,\n",
    "    trajectory_encoder=trajectory_encoder,\n",
    ")\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define train config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.config import TrainConfig\n",
    "\n",
    "config = TrainConfig(\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    datasets={ 'train': train_set, 'valid': valid_set },\n",
    "    n_epoch=5,\n",
    "    all_candidates=all_candidates,\n",
    "    verbose=True,\n",
    "    cuda=USE_CUDA,\n",
    "    checkpoint=checkpoint,\n",
    "    preprocess=preprocess,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] 1:   0%|          | 0/6382 [00:00<?, ?it/s]c:\\Users\\chinp\\source\\capstone-project-9900w16aaiadvance\\venv\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "[train] 1: 100%|██████████| 6382/6382 [08:56<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.34502483012673624\n",
      "elapsed: 536.3130717277527\n",
      "perplexity: 1.4120249799915892\n",
      "accuracy: 0.9278285370665228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[valid] 1: 100%|██████████| 1596/1596 [01:21<00:00, 19.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdev: 1459.11754277356\n",
      "elapsed: 81.43980669975281\n",
      "perplexity: 1.267627244256125\n",
      "accuracy: 0.9458875808127243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test] 1: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 1.0\n",
      "accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] 2: 100%|██████████| 6382/6382 [09:03<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.22957574169054296\n",
      "elapsed: 543.142297744751\n",
      "perplexity: 1.2580661516718912\n",
      "accuracy: 0.9459330160783698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[valid] 2: 100%|██████████| 1596/1596 [01:15<00:00, 21.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdev: 721.1150676564764\n",
      "elapsed: 75.76587390899658\n",
      "perplexity: 1.2557706730673222\n",
      "accuracy: 0.9464186935272432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test] 2: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 0.004126310348510742\n",
      "perplexity: 1.0\n",
      "accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] 3: 100%|██████████| 6382/6382 [08:28<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.22201307219155753\n",
      "elapsed: 508.6900722980499\n",
      "perplexity: 1.248587699535188\n",
      "accuracy: 0.9463755957985402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[valid] 3: 100%|██████████| 1596/1596 [01:15<00:00, 21.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdev: 720.6402325355319\n",
      "elapsed: 75.07411313056946\n",
      "perplexity: 1.2476887616196553\n",
      "accuracy: 0.9464558143364755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test] 3: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 0.0016241073608398438\n",
      "perplexity: 1.0\n",
      "accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] 4: 100%|██████████| 6382/6382 [08:55<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.21864594555445616\n",
      "elapsed: 535.2086660861969\n",
      "perplexity: 1.2443906166534933\n",
      "accuracy: 0.9465386186580753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[valid] 4: 100%|██████████| 1596/1596 [01:17<00:00, 20.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdev: 720.5645127643021\n",
      "elapsed: 77.57998180389404\n",
      "perplexity: 1.2455669457648562\n",
      "accuracy: 0.9464594855867233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test] 4: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 0.0019922256469726562\n",
      "perplexity: 1.0\n",
      "accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] 5: 100%|██████████| 6382/6382 [08:50<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.21696043086673952\n",
      "elapsed: 530.0546660423279\n",
      "perplexity: 1.2422949446309977\n",
      "accuracy: 0.9465394417849576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[valid] 5: 100%|██████████| 1596/1596 [01:19<00:00, 20.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdev: 720.5732124156522\n",
      "elapsed: 79.28772377967834\n",
      "perplexity: 1.2434330681908632\n",
      "accuracy: 0.9464592816387501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[test] 5: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 0.0005626678466796875\n",
      "perplexity: 1.0\n",
      "accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.ml.train import train\n",
    "\n",
    "if USE_CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "state = train(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "\n",
    "check accuracy if we just predict the next position as the last known position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[valid] 6: 100%|██████████| 1596/1596 [00:27<00:00, 59.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9667665488976879\n",
      "143.26925145636451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from src.ml.utils import haversine\n",
    "\n",
    "valid_count = 0\n",
    "valid_acc = 0\n",
    "valid_mse = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for trajectories, context, target in tqdm.tqdm(state.valid_loader, desc=state.get_tqdm_desc('[valid]'), disable=not config.verbose):\n",
    "        batch_size, sequence_length = trajectories.shape[:2]\n",
    "    \n",
    "        context: torch.FloatTensor = context\n",
    "        trajectories: torch.FloatTensor = trajectories\n",
    "        target: torch.IntTensor = target\n",
    "\n",
    "        if config.cuda:\n",
    "            context = context.cuda()\n",
    "            trajectories = trajectories.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        if config.preprocess:\n",
    "            trajectories = config.preprocess(trajectories)\n",
    "\n",
    "        trajectories = torch.concat((trajectories[:, :1, :], trajectories), dim=1)\n",
    "\n",
    "        # accumulate accuracy\n",
    "        acc = (trajectories[:, :-1] == trajectories[:, 1:]).prod(dim=-1).float().mean().item()\n",
    "        valid_acc += batch_size * acc\n",
    "\n",
    "        mdev = haversine(trajectories[:, 1:], trajectories[:, :-1]).mean().item()\n",
    "        valid_mse += batch_size * mdev\n",
    "\n",
    "        valid_count += batch_size\n",
    "\n",
    "print(valid_acc / valid_count)\n",
    "print(valid_mse / valid_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment results:\n",
    "| Dimension | Perplexity | Mean Error (m) | Accuracy |\n",
    "|-----------|------------|------------|----------|\n",
    "| 16 | 1.32 | 983 | 0.9459924913363946 |\n",
    "| 32 | 1.28 | 758 | 0.9464138739912731 |\n",
    "| 64 | 1.25 | 726 | 0.9464254994738969 |\n",
    "| 128 | 1.24 | 721 | 0.9464592816387501 |\n",
    "| last position | - | 143 | 0.9667665488976879 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
